# Enhanced Blind-Watermarking-Based IP Protection Technique
Official PyTorch implementation for paper [Can Image Watermarking Efficiently Protect Deep‑Learning‑Based Image Classifiers? – A Preliminary Security Analysis of an IP‑Protecting Method](https://link.springer.com/chapter/10.1007/978-3-031-36574-4_3)



## Requirements

You need [PyTorch](https://pytorch.org/) 1.1 with torchvision and external packages to set up. To visualize the training process, you should install TensorboardX and Tensorboard.  However, this is optional. All the experiments were conducted on Ubuntu 20.04.2 LTS (Focal Fossa).



## Data

We conducted experiments on mini-ImageNet dataset created by an [open-source tool](https://github.com/yaoyao-liu/mini-imagenet-tools). The dataset involves 60,000 256×256 images, evenly drawn from 100 classes. Note that extra self-made datasets (images prepared for training various versions of steganography and steganalysis algorithms, counterfeit stego images, distorted triggers, etc.) are required. For image classification, following the usual academic practice, we normally allocated 80% of that to training, 10% to validation and remaining 10% to testing. When performing watermarking training, a small number of triggers (e.g. 1% of training set) were mixed into batches of plain data to leave a backdoor in the host DNN.

The directory of mini-ImageNet dataset has the following structure so that we can use the standard torchvision data loaders without changes:

```
<data_root>/
    class_A/
        class_A_image1.jpg
        class_A_image2.jpg
        ...
    class_B/
        class_B_image1.jpg
        class_B_image2.jpg
        ...
    ...
```

When training steganography algorithms, we randomly extracted several classes from mini-ImageNet to construct another dataset for the task, half of which were used as cover images while the remaining served as secret images. Therefore, the structure of data is slightly different from that of mini-ImageNet:

```
<data_root>/
	cover_data/
		class_A/
            class_A_image1.jpg
            class_A_image2.jpg
            ...
	secret_data/
	    class_B/
            class_B_image1.jpg
            class_B_image2.jpg
            ...
```

When training steganalysis algorithms, the dataset should be generated by a range of pretrained steganography algorithms. Note that the data is subject to be trained in pairs strictly. One possible structure can be:

```
<data_root>/
	train/
		cover_images/
			image1.jpg
			image2.jpg
			...
		secret_images/
			image1.jpg
			image2.jpg
			...
	validation/
		cover_images/
			image1.jpg
			image2.jpg
			...
		secret_images/
			image1.jpg
			image2.jpg
			...
```




## Running experiments
The project is mainly divided into four separate folders instead of a super complex one, which basically share the same structure so as to be managed in a unified manner. Typically, there are a couple of executable files (.ipynb) in every subproject. To run experiments in parallel, we duplicated some files with minor but crucial changes (e.g. diverse algorithms). Note that the codes for three security attacks are attached to different subprojects respectively.

### The original blind-watermarking-based IP protection framework

- To evaluate ACSAC19 on a more practically sized image dataset, you may run ACSAC19_ImageNet.ipynb in ./Blind-Watermarking-Based_IP_Protection_Technique_enhanced_version folder.
- Since the ACSAC19 mainly conducted experiments on [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset, the original data loader is not applicable to mini-ImageNet any more, so we have made changes to it.
- To better visualize the training process and save experimental results, we have also appended additional functions in it. Other than that, the re-implementation strictly follow the original work.

### The enhanced blind-watermarking-based IP protection frameworks

- To verify the effectiveness of two enhanced versions, you may run E2E-Extraction.ipynb or Two-Phase.ipynb in the same directory.
- For end-to-end training strategy, as long as  E2E-Extraction.ipynb starts running, the steganography algorithm, image extraction algorithm and host DNN are able to be trained from scratch simultaneously on mini-ImageNet, which means these modules will interact with each other all the while.
- For two-phase approach, you are supposed to pretrain host DNN until convergence in advance using main.ipynb in ./host_resnet18 folder. Meanwhile, run train_En2D.ipynb or train_GglNet.ipynb in ./train_steganography to train a pair of steganography algorithm and corresponding image extraction algorithm, which are used to generate triggers for watermarking training. After the preparation phase, fine tune the pretrained host DNN on the dataset consisting of plain images and stego images with Two-Phase.ipynb in ./Blind-Watermarking-Based_IP_Protection_Technique_enhanced_version folder.

### Evasion attacks

- Attackers leverage steganalyzers to detect and remove possible triggers during transmission, hindering model owners' verification query.
- Acting as an attacker with different levers of capabilities, you need to train steganalyzers with self-designed datasets, which are generated by several types of steganography models initialized by multiple random seeds. The motivation is to explore the key (e.g. sharing steganography algorithms or model weights) to encourage detection rates of steganalyzers. The implementation can be found in evasion_attack_counterfeit.ipynb or evasion_attack_triggers.ipynb in ./train_steganalysis folder.

### Spoofing attacks

- Spoofing attacks include two cases: in the first case, attackers may send counterfeit triggers to host DNN in an attempt to acquire high query accuracy. In the second case, some of them with access to a subset of real triggers are likely to illegally extract secret images. Both actions are potential threats to model ownership.
- To realize the first case, you need to prepare steganography algorithms trained with a series of random seeds and generate various sets of counterfeit triggers. Afterwards, feed them into the watermarked model following the steps in spoofing_attack_verify_accuracy.ipynb in ./host_resnet18. You can check the recognition rates reported in our paper.
- To realize the second case, you need to deliberately replace the matching image extraction algorithm (checkpoint) with one that NOT corresponding to the original steganography algorithm, examining whether the attackers' plot still work or not. Detailed implementation have been provided in the file spoofing_attack_En2D_extract_wm.ipynb (or spoofing_attack_GglNet_extract_wm.ipynb) in ./train_steganography folder. You can refer to more intuitive results presented in our paper.

### Robustness attacks

- Aimed at frustrating the ownership verification task, attackers may add additive noise to stego images to prevent host DNN from recognizing triggers.
- To launch robustness attacks, you need to input triggers, which are distorted by Gaussian noise and salt & pepper noise, into owner's model and record success rates. Note that the file robustness_attack.ipynb in ./host_resnet18 performs a similar task as the first case of spoofing attacks, but it carries a distinct objective.



## Citation

If you find our contribution useful in your research, please consider to cite the papers:

```BibTeX
@inproceedings{10.1007/978-3-031-36574-4_3,
author = {Jia‑Hui Xie, Di Wu, Bo‑Hao Zhang, Hai Su and Huan Yang},
title = {Can Image Watermarking Efficiently Protect Deep-Learning-Based Image Classifiers? – A Preliminary Security Analysis of an IP-Protecting Method},
year = {2023},
isbn = {978-3-031-36574-4},
publisher = {Springer, Cham},
url = {https://doi.org/10.1007/978-3-031-36574-4_3},
doi = {10.1007/978-3-031-36574-4_3},
booktitle = {Digital Forensics and Cyber Crime},
pages = {34-57},
numpages = {24}
}
```



## Acknowledgments

- [Tools for mini-ImageNet Dataset](https://github.com/yaoyao-liu/mini-imagenet-tools)
- [How to Prove Your Model Belongs to You: A Blind-Watermark based Framework to Protect Intellectual Property of DNN](https://github.com/zhenglisec/Blind-Watermark-for-DNN)
- [Deep Residual Network for Steganalysis of Digital Images (SRNet model) Pytorch Implementation](https://github.com/brijeshiitg/Pytorch-implementation-of-SRNet)
- [YeNet-Pytorch](https://github.com/Caenorst/YeNet-Pytorch)
- [End-To-End-Image-Steganography](https://github.com/qzramiz/End-To-End-Image-Steganography)

- [DeepSteganography](https://github.com/krishvishal/DeepSteganography)