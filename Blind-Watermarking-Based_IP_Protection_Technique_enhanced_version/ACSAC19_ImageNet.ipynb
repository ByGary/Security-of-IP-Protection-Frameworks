{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a68bc9-c3b8-4aa1-b744-7c5ea4b244a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.optim.lr_scheduler import MultiStepLR, ReduceLROnPlateau\n",
    "from collections import defaultdict\n",
    "\n",
    "from network.Discriminator import DiscriminatorNet\n",
    "from network.HidingUNet import UnetGenerator\n",
    "from data.load_data import get_loader\n",
    "from utils.AverageMeter import AverageMeter\n",
    "from utils.SSIM import SSIM\n",
    "from utils.helper import *\n",
    "\n",
    "'''\n",
    "def SpecifiedLabel(OriginalLabel):\n",
    "    targetlabel = OriginalLabel + 1\n",
    "    targetlabel = targetlabel % 10\n",
    "    return targetlabel\n",
    "'''\n",
    "GPU = '0'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = GPU\n",
    "cfg = load_config()\n",
    "run_folder = create_folder_acsac(cfg.save_path)\n",
    "# Print the configuration.\n",
    "logging.basicConfig(level=logging.INFO, format='%(message)s', handlers=[logging.FileHandler(os.path.join(run_folder, f'run.log')), logging.StreamHandler(sys.stdout)])\n",
    "logging.info(\"Experiment Configuration:\")\n",
    "logging.info(\"CUDA_VISIBLE_DEVICESï¼š{}\".format(os.getenv('CUDA_VISIBLE_DEVICES')))\n",
    "logging.info(cfg)\n",
    "logging.info(\"run_folder:{}\".format(run_folder))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    cudnn.benchmark = True\n",
    "    if cfg.seed is not None:\n",
    "        np.random.seed(cfg.seed)\n",
    "        random.seed(cfg.seed)\n",
    "        torch.manual_seed(cfg.seed)\n",
    "        torch.cuda.manual_seed(cfg.seed)\n",
    "        torch.cuda.manual_seed_all(cfg.seed)\n",
    "        cudnn.deterministic = True\n",
    "        '''\n",
    "        warnings.warn('You have choosen to seed training. '\n",
    "                      'This will turn on the CUDNN deterministic setting, '\n",
    "                      'which can slow down your training considerably! '\n",
    "                      'You may see unexpected behavior when restarting '\n",
    "                      'from checkpoints.')\n",
    "        '''\n",
    "\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "train_loader, val_loader, test_loader, cover_loader = get_loader(cfg, 'train'), get_loader(cfg, 'val'), get_loader(cfg, 'test'), get_loader(cfg, 'original_work')\n",
    "logging.info(\"train_loader:{} val_loader:{} test_loader:{}\\n\".format(len(train_loader.dataset), len(val_loader.dataset), len(test_loader.dataset)))\n",
    "\n",
    "# logo\n",
    "transform_test = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "mini_logo = torchvision.datasets.ImageFolder(\n",
    "    root='./data/IEEE', transform=transform_test)\n",
    "mini_loader = torch.utils.data.DataLoader(mini_logo, batch_size=1)\n",
    "\n",
    "for _, (logo, __) in enumerate(mini_loader):\n",
    "    secret_img = logo.expand(cfg.wm_batchsize, logo.shape[1], logo.shape[2], logo.shape[3]).cuda()\n",
    "\n",
    "np_labels = np.random.randint(100, size=(int(cfg.wm_num[0]/cfg.wm_batchsize), cfg.wm_batchsize))\n",
    "wm_labels = torch.from_numpy(np_labels).cuda()\n",
    "logging.info(\"wm_labels:{}\".format(wm_labels))\n",
    "# load the 1% origin sample\n",
    "# get the watermark-cover pairs for each batch\n",
    "wm_inputs, wm_cover_labels = [], []\n",
    "# wm_labels = []\n",
    "if cfg.wm_train:\n",
    "    for wm_idx, (wm_input, wm_cover_label) in enumerate(cover_loader):\n",
    "        wm_input, wm_cover_label = wm_input.cuda(), wm_cover_label.cuda()\n",
    "        wm_inputs.append(wm_input)\n",
    "        wm_cover_labels.append(wm_cover_label)\n",
    "        #wm_labels.append(SpecifiedLabel(wm_cover_label))\n",
    "\n",
    "        if wm_idx == (int(cfg.wm_num[0]/cfg.wm_batchsize)-1):  # Choose 1% dataset as origin samples.\n",
    "            break\n",
    "\n",
    "# adversarial ground truths\n",
    "valid = torch.cuda.FloatTensor(cfg.wm_batchsize, 1).fill_(1.0)\n",
    "fake = torch.cuda.FloatTensor(cfg.wm_batchsize, 1).fill_(0.0)\n",
    "\n",
    "# wm_labels = SpecifiedLabel()\n",
    "best_real_acc, best_wm_acc, best_wm_input_acc = 0, 0, 0\n",
    "start_epoch = 0\n",
    "\n",
    "# model\n",
    "Hidnet = UnetGenerator()\n",
    "Disnet = DiscriminatorNet()\n",
    "Dnnet = resnet18()\n",
    "Hidnet = nn.DataParallel(Hidnet.cuda())\n",
    "Disnet = nn.DataParallel(Disnet.cuda())\n",
    "Dnnet = nn.DataParallel(Dnnet.cuda())\n",
    "\n",
    "criterionH_mse = nn.MSELoss()\n",
    "criterionH_ssim = SSIM()\n",
    "optimizerH = optim.Adam(Hidnet.parameters(), lr=cfg.lr[0], betas=(0.5, 0.999))\n",
    "schedulerH = ReduceLROnPlateau(optimizerH, mode='min', factor=0.2, patience=5, verbose=True)\n",
    "\n",
    "criterionD = nn.BCELoss()\n",
    "optimizerD = optim.Adam(Disnet.parameters(), lr=cfg.lr[0], betas=(0.5, 0.999))\n",
    "schedulerD = ReduceLROnPlateau(optimizerD, mode='min', factor=0.2, patience=8, verbose=True)\n",
    "\n",
    "criterionN = nn.CrossEntropyLoss()\n",
    "optimizerN = optim.SGD(Dnnet.parameters(), lr=cfg.lr[1], momentum=0.9, weight_decay=5e-4)\n",
    "schedulerN = MultiStepLR(optimizerN, milestones=[40, 80], gamma=0.1)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    epoch_start_time = time.time()\n",
    "    logging.info('\\nEpoch: %d' % epoch)\n",
    "    Dnnet.train()\n",
    "    Hidnet.train()\n",
    "    Disnet.train()\n",
    "\n",
    "    step = 1\n",
    "    total_duration = 0\n",
    "    wm_cover_correct, wm_correct, real_correct, wm_total, real_total, real_dis_correct, fake_dis_correct, dis_total = 0, 0, 0, 0, 0, 0, 0, 0\n",
    "    loss_H_ = AverageMeter()\n",
    "    loss_D_ = AverageMeter()\n",
    "    loss_mse_ = AverageMeter()\n",
    "    loss_ssim_ = AverageMeter()\n",
    "    loss_DNN_ = AverageMeter()\n",
    "    loss_real_ = AverageMeter()\n",
    "    real_acc = AverageMeter()\n",
    "    wm_acc = AverageMeter()\n",
    "    dis_acc = AverageMeter()\n",
    "\n",
    "    # Save relevant metrics in dictionaries.\n",
    "    losses_dict = defaultdict(AverageMeter)\n",
    "    metrics_dict = defaultdict(AverageMeter)\n",
    "\n",
    "    for batch_idx, (input, label) in enumerate(train_loader):\n",
    "        input, label = input.cuda(), label.cuda()\n",
    "        wm_input = wm_inputs[(wm_idx + batch_idx) % len(wm_inputs)]\n",
    "        wm_label = wm_labels[(wm_idx + batch_idx) % len(wm_inputs)]\n",
    "        if batch_idx == 0:\n",
    "            logging.info(\"wm_label:{}\".format(wm_label))\n",
    "        wm_cover_label = wm_cover_labels[(wm_idx + batch_idx) % len(wm_inputs)]\n",
    "        #############Discriminator##############\n",
    "        optimizerD.zero_grad()\n",
    "        wm_img = Hidnet(wm_input, secret_img)\n",
    "        wm_dis_output = Disnet(wm_img.detach())\n",
    "        real_dis_output = Disnet(wm_input)\n",
    "\n",
    "        fake_dis_pred = wm_dis_output.lt(0.5)\n",
    "        real_dis_pred = real_dis_output.gt(0.5)\n",
    "\n",
    "        loss_D_wm = criterionD(wm_dis_output, fake)\n",
    "        loss_D_real = criterionD(real_dis_output, valid)\n",
    "        loss_D = loss_D_wm + loss_D_real\n",
    "        loss_D.backward()\n",
    "        optimizerD.step()\n",
    "        ################Hidding Net#############\n",
    "        optimizerH.zero_grad()\n",
    "        optimizerD.zero_grad()\n",
    "        optimizerN.zero_grad()\n",
    "        wm_dis_output = Disnet(wm_img)\n",
    "        wm_dnn_output = Dnnet(wm_img)\n",
    "        loss_mse = criterionH_mse(wm_input, wm_img)\n",
    "        loss_ssim = criterionH_ssim(wm_input, wm_img)\n",
    "        loss_adv = criterionD(wm_dis_output, valid)\n",
    "\n",
    "        loss_dnn = criterionN(wm_dnn_output, wm_label)\n",
    "        loss_H = cfg.hyper_parameters[0] * loss_mse + cfg.hyper_parameters[1] * (1-loss_ssim) + cfg.hyper_parameters[2] * loss_adv + cfg.hyper_parameters[3] * loss_dnn\n",
    "        loss_H.backward()\n",
    "        optimizerH.step()\n",
    "        ################DNNet#############\n",
    "        optimizerN.zero_grad()\n",
    "        inputs = torch.cat([input, wm_img.detach()], dim=0)\n",
    "        labels = torch.cat([label, wm_label], dim=0)\n",
    "        dnn_output = Dnnet(inputs)\n",
    "\n",
    "        loss_DNN = criterionN(dnn_output, labels)\n",
    "        loss_DNN.backward()\n",
    "        optimizerN.step()\n",
    "\n",
    "        # calculate the accuracy\n",
    "        wm_cover_output = Dnnet(wm_input)\n",
    "        _, wm_cover_predicted = wm_cover_output.max(1)\n",
    "        wm_cover_correct += wm_cover_predicted.eq(wm_cover_label).sum().item()\n",
    "\n",
    "        _, wm_predicted = dnn_output[cfg.batchsize: cfg.batchsize + cfg.wm_batchsize].max(1)\n",
    "        wm_correct += wm_predicted.eq(wm_label).sum().item()\n",
    "        wm_total += cfg.wm_batchsize\n",
    "\n",
    "        _, real_predicted = dnn_output[0:cfg.batchsize].max(1)\n",
    "        real_correct += real_predicted.eq(labels[0:cfg.batchsize]).sum().item()\n",
    "        real_total += cfg.batchsize\n",
    "\n",
    "        real_dis_correct += real_dis_pred.eq(valid).sum().item()\n",
    "        fake_dis_correct += fake_dis_pred.eq(fake).sum().item()\n",
    "        dis_total += 2 * cfg.wm_batchsize\n",
    "\n",
    "        loss_real = criterionN(dnn_output[0:cfg.batchsize], label)\n",
    "\n",
    "        loss_H_.update(loss_H.item(), int(wm_input.size()[0]))\n",
    "        loss_D_.update(loss_D.item(), int(wm_input.size()[0]))\n",
    "        loss_mse_.update(loss_mse.item(), int(wm_input.size()[0]))\n",
    "        loss_ssim_.update(loss_ssim.item(), int(wm_input.size()[0]))\n",
    "        loss_DNN_.update(loss_DNN.item(), int(inputs.size()[0]))\n",
    "        loss_real_.update(loss_real.item(), int(input.size()[0]))\n",
    "\n",
    "        real_acc.update(100. * real_correct / real_total)\n",
    "        wm_acc.update(100. * wm_correct / wm_total)\n",
    "        dis_acc.update(100. * (real_dis_correct + fake_dis_correct) / dis_total)\n",
    "\n",
    "        if step % cfg.print_freq == 0 or step == (len(train_loader)):\n",
    "            logging.info('[%d/%d][%d/%d]  Loss_D: %.4f Loss_H: %.4f (mse: %.4f ssim: %.4f adv: %.4f)  Loss_DNN: %.4f loss_real: %.4f Real acc: %.3f  wm acc: %.3f dis_acc: %.3f' % (\n",
    "                epoch, cfg.num_epochs, step, len(train_loader),\n",
    "                loss_D.item(), loss_H.item(), loss_mse.item(), loss_ssim.item(), loss_adv.item(), loss_DNN.item(), loss_real.item(),\n",
    "                100. * real_correct / real_total, 100. * wm_correct / wm_total, dis_acc.avg))\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    losses_dict['loss_D'] = loss_D_.avg\n",
    "    losses_dict['loss_H'] = loss_H_.avg\n",
    "    losses_dict['loss_mse'] = loss_mse_.avg\n",
    "    losses_dict['ssim'] = loss_ssim_.avg\n",
    "    losses_dict['loss_DNN'] = loss_DNN_.avg\n",
    "    losses_dict['loss_real'] = loss_real_.avg\n",
    "\n",
    "    metrics_dict['real_acc'] = real_acc.avg\n",
    "    metrics_dict['wm_acc'] = wm_acc.avg\n",
    "    metrics_dict['dis_acc'] = dis_acc.avg\n",
    "\n",
    "    total_duration = time.time() - epoch_start_time\n",
    "    logging.info('Epoch {} total duration: {:.2f} sec'.format(epoch, total_duration))\n",
    "    logging.info('-' * 130)\n",
    "\n",
    "    write_scalars_acsac(epoch, os.path.join(run_folder, 'train.csv'), losses_dict, metrics_dict, total_duration)\n",
    "\n",
    "    return losses_dict, metrics_dict\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    epoch_start_time = time.time()\n",
    "    Dnnet.eval()\n",
    "    Hidnet.eval()\n",
    "    Disnet.eval()\n",
    "    global best_real_acc\n",
    "    global best_wm_acc\n",
    "\n",
    "    total_duration = 0\n",
    "    wm_cover_correct, wm_correct, real_correct, real_total, wm_total, real_dis_correct, fake_dis_correct, dis_total = 0, 0, 0, 0, 0, 0, 0, 0\n",
    "    loss_H_ = AverageMeter()\n",
    "    loss_D_ = AverageMeter()\n",
    "    loss_mse_ = AverageMeter()\n",
    "    loss_ssim_ = AverageMeter()\n",
    "    loss_DNN_ = AverageMeter()\n",
    "    loss_real_ = AverageMeter()\n",
    "\n",
    "    real_acc_ = AverageMeter()\n",
    "    wm_acc_ = AverageMeter()\n",
    "    dis_acc_ = AverageMeter()\n",
    "    ste_psnr_ = AverageMeter()\n",
    "\n",
    "    # Save relevant metrics in dictionaries.\n",
    "    losses_dict = defaultdict(AverageMeter)\n",
    "    metrics_dict = defaultdict(AverageMeter)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (input, label) in enumerate(val_loader):\n",
    "            input, label = input.cuda(), label.cuda()\n",
    "            wm_input = wm_inputs[(wm_idx + batch_idx) % len(wm_inputs)]\n",
    "            wm_label = wm_labels[(wm_idx + batch_idx) % len(wm_inputs)]\n",
    "            if batch_idx == 0:\n",
    "                logging.info(\"wm_label:{}\".format(wm_label))\n",
    "            wm_cover_label = wm_cover_labels[(wm_idx + batch_idx) % len(wm_inputs)]\n",
    "            #############Discriminator###############\n",
    "            wm_img = Hidnet(wm_input, secret_img)\n",
    "            wm_dis_output = Disnet(wm_img.detach())\n",
    "            real_dis_output = Disnet(wm_input)\n",
    "\n",
    "            fake_dis_pred = wm_dis_output.lt(0.5)\n",
    "            real_dis_pred = real_dis_output.gt(0.5)\n",
    "\n",
    "            loss_D_wm = criterionD(wm_dis_output, fake)\n",
    "            loss_D_real = criterionD(real_dis_output, valid)\n",
    "            loss_D = loss_D_wm + loss_D_real\n",
    "\n",
    "            ################Hidding Net#############\n",
    "            wm_dnn_outputs = Dnnet(wm_img)\n",
    "            loss_mse = criterionH_mse(wm_input, wm_img)\n",
    "            loss_ssim = criterionH_ssim(wm_input, wm_img)\n",
    "            loss_adv = criterionD(wm_dis_output, valid)\n",
    "\n",
    "            loss_dnn = criterionN(wm_dnn_outputs, wm_label)\n",
    "            loss_H = cfg.hyper_parameters[0] * loss_mse + cfg.hyper_parameters[1] * (1-loss_ssim) + cfg.hyper_parameters[2] * loss_adv + cfg.hyper_parameters[3] * loss_dnn\n",
    "            ################DNNet#############\n",
    "            inputs = torch.cat([input, wm_img.detach()], dim=0)\n",
    "            labels = torch.cat([label, wm_label], dim=0)\n",
    "            dnn_outputs = Dnnet(inputs)\n",
    "\n",
    "            loss_DNN = criterionN(dnn_outputs, labels)\n",
    "\n",
    "            wm_cover_output = Dnnet(wm_input)\n",
    "            _, wm_cover_predicted = wm_cover_output.max(1)\n",
    "            wm_cover_correct += wm_cover_predicted.eq(\n",
    "                wm_cover_label).sum().item()\n",
    "\n",
    "            _, wm_predicted = dnn_outputs[cfg.batchsize:\n",
    "                                          cfg.batchsize + cfg.wm_batchsize].max(1)\n",
    "            wm_correct += wm_predicted.eq(wm_label).sum().item()\n",
    "            wm_total += cfg.wm_batchsize\n",
    "\n",
    "            _, real_predicted = dnn_outputs[0:cfg.batchsize].max(1)\n",
    "            real_correct += real_predicted.eq(\n",
    "                labels[0:cfg.batchsize]).sum().item()\n",
    "            real_total += cfg.batchsize\n",
    "\n",
    "            real_dis_correct += real_dis_pred.eq(valid).sum().item()\n",
    "            fake_dis_correct += fake_dis_pred.eq(fake).sum().item()\n",
    "            dis_total += 2 * cfg.wm_batchsize\n",
    "\n",
    "            loss_real = criterionN(dnn_outputs[0:cfg.batchsize], label)\n",
    "            ste_psnr = cal_psnr(wm_input, wm_img.detach())\n",
    "\n",
    "            loss_D_.update(loss_D.item(), int(wm_input.size()[0]))\n",
    "            loss_H_.update(loss_H.item(), int(wm_input.size()[0]))\n",
    "            loss_mse_.update(loss_mse.item(), int(wm_input.size()[0]))\n",
    "            loss_ssim_.update(loss_ssim.item(), int(wm_input.size()[0]))\n",
    "            loss_DNN_.update(loss_DNN.item(), int(inputs.size()[0]))\n",
    "            loss_real_.update(loss_real.item(), int(input.size()[0]))\n",
    "\n",
    "            real_acc_.update(100. * real_correct / real_total)\n",
    "            wm_acc_.update(100. * wm_correct / wm_total)\n",
    "            dis_acc_.update(100. * (real_dis_correct + fake_dis_correct) / dis_total)\n",
    "            ste_psnr_.update(ste_psnr)\n",
    "\n",
    "    logging.info('Loss_D: %.4f Loss_H: %.4f (mse: %.4f ssim: %.4f adv: %.4f) Loss_DNN: %.4f  loss_real: %.4f Real acc: %.3f  wm acc: %.3f dis_acc: %.3f ste_psnr: %.3f' % (\n",
    "        loss_D.item(), loss_H.item(), loss_mse.item(), loss_ssim.item(), loss_adv.item(), loss_DNN.item(), loss_real.item(), \n",
    "        100. * real_correct / real_total, 100. * wm_correct / wm_total, dis_acc_.avg, ste_psnr_.avg))\n",
    "\n",
    "    losses_dict['loss_D'] = loss_D_.avg\n",
    "    losses_dict['loss_H'] = loss_H_.avg\n",
    "    losses_dict['loss_mse'] = loss_mse_.avg\n",
    "    losses_dict['ssim'] = loss_ssim_.avg\n",
    "    losses_dict['loss_DNN'] = loss_DNN_.avg\n",
    "    losses_dict['loss_real'] = loss_real_.avg\n",
    "\n",
    "    metrics_dict['real_acc'] = real_acc_.avg\n",
    "    metrics_dict['wm_acc'] = wm_acc_.avg\n",
    "    metrics_dict['dis_acc'] = dis_acc_.avg\n",
    "    metrics_dict['ste_psnr'] = ste_psnr_.avg\n",
    "\n",
    "    real_acc = 100. * real_correct / real_total\n",
    "    wm_acc = 100. * wm_correct / wm_total\n",
    "\n",
    "    if real_acc >= best_real_acc:  # and (wm_acc >= best_wm_acc):\n",
    "        save_all_models_acsac(epoch, run_folder, Hidnet, Disnet, Dnnet, optimizerH, optimizerD, optimizerN,\n",
    "            losses_dict, metrics_dict, wm_labels)\n",
    "        best_real_acc = real_acc\n",
    "        logging.info(\"best_real_acc:{}\".format(best_real_acc))\n",
    "\n",
    "    if wm_acc > best_wm_acc:\n",
    "        best_wm_acc = wm_acc\n",
    "        logging.info(\"best_wm_acc:{}\".format(best_wm_acc))\n",
    "\n",
    "    total_duration = time.time() - epoch_start_time\n",
    "    write_scalars_acsac(epoch, os.path.join(run_folder, 'val.csv'), losses_dict, metrics_dict, total_duration)\n",
    "    save_cat_image_acsac(cfg, epoch, run_folder, wm_input, wm_img, secret_img, None, 'val')\n",
    "    logging.info('Epoch {} total duration: {:.2f} sec'.format(epoch, total_duration))\n",
    "\n",
    "    return losses_dict, metrics_dict\n",
    "\n",
    "\n",
    "for epoch in range(cfg.num_epochs):\n",
    "    train_losses_dict, train_metrics_dict = train(epoch)\n",
    "    val_losses_dict, val_metrics_dict = test(epoch)\n",
    "    plot_scalars(epoch, run_folder, train_losses_dict, train_metrics_dict, val_losses_dict, val_metrics_dict, None)\n",
    "    schedulerH.step(val_losses_dict['loss_H'])\n",
    "    schedulerD.step(val_losses_dict['loss_D'])\n",
    "    schedulerN.step()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
