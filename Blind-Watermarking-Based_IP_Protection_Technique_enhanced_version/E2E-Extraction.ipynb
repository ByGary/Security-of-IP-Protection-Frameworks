{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e4753a-4e23-4b4d-97ed-a69766431655",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import warnings\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import defaultdict, OrderedDict\n",
    "from torchmetrics import Accuracy, Precision, Recall, F1\n",
    "\n",
    "from data.load_data import *\n",
    "# Select the steganography algorithm you want.\n",
    "from network.End_to_end_Ste import End_to_end_Ste\n",
    "# from network.GoogleNet import GoogleNet\n",
    "from models.resnet import resnet18\n",
    "from utils.AverageMeter import AverageMeter\n",
    "from utils.helper import *\n",
    "from utils.pytorchtools import E2E_EarlyStopping\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "cfg = load_config()\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "device = torch.device(cfg.system.device)\n",
    "run_folder = create_folder(cfg.results.run_folder)\n",
    "# Print the configuration.\n",
    "logging.basicConfig(level=logging.INFO, format='%(message)s', handlers=[logging.FileHandler(os.path.join(run_folder, f'run.log')), logging.StreamHandler(sys.stdout)])\n",
    "logging.info(\"Experiment Configuration:\")\n",
    "logging.info(\"CUDA_VISIBLE_DEVICES：{}\".format(os.getenv('CUDA_VISIBLE_DEVICES')))\n",
    "logging.info(cfg)\n",
    "logging.info(\"run_folder:{}\".format(run_folder))\n",
    "# to be reproducible\n",
    "if torch.cuda.is_available():\n",
    "    cudnn.benchmark = True\n",
    "    if cfg.train.seed is not None:\n",
    "        np.random.seed(cfg.train.seed)  # Numpy module.\n",
    "        random.seed(cfg.train.seed)  # Python random module.\n",
    "        torch.manual_seed(cfg.train.seed)  # Sets the seed for generating random numbers.\n",
    "        torch.cuda.manual_seed(cfg.train.seed)  # Sets the seed for generating random numbers for the current GPU.\n",
    "        torch.cuda.manual_seed_all(cfg.train.seed)  # Sets the seed for generating random numbers on all GPUs.\n",
    "        cudnn.deterministic = True\n",
    "\n",
    "        warnings.warn('You have choosen to seed training. '\n",
    "                      'This will turn on the CUDNN deterministic setting, '\n",
    "                      'which can slow down your training considerably! '\n",
    "                      'You may see unexpected behavior when restarting '\n",
    "                      'from checkpoints.')\n",
    "\n",
    "# data loaders\n",
    "train_loader, val_loader, test_loader = get_loader(cfg, 'train'), get_loader(cfg, 'val'), get_loader(cfg, 'test')\n",
    "logging.info(\"train_loader:{} val_loader:{} test_loader:{}\\n\".format(len(train_loader.dataset), len(val_loader.dataset), len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "def load_pretrained():\n",
    "\n",
    "    if cfg.train.pretrained_tech is True:\n",
    "        hid_ckp_path = ''  # Fill in the checkpoint of steganography(e.g. En2D or GglNet).\n",
    "        hid_state_dicts = torch.load(hid_ckp_path)\n",
    "        stegan_state_dict = hid_state_dicts['']  # model weights\n",
    "        stegan_opt_dict = hid_state_dicts['']  # parameters of the optimizer\n",
    "        ste_model.load_state_dict(stegan_state_dict)\n",
    "        optimizerH.load_state_dict(stegan_opt_dict)\n",
    "\n",
    "        pg_dict = stegan_opt_dict['param_groups'][0]\n",
    "        logging.info(\"stegan_opt_dict: lr:{} betas: {} eps: {} weight_decay:{}\".format(\n",
    "            pg_dict['lr'], pg_dict['betas'], pg_dict['eps'], pg_dict['weight_decay']))\n",
    "        pg = optimizerH.param_groups[0]\n",
    "        logging.info(\"optimizerH: lr:{} betas: {} eps: {} weight_decay:{}\".format(\n",
    "            pg['lr'],  pg['betas'], pg['eps'], pg['weight_decay']))\n",
    "        logging.info(\"Have loaded pretrained stegan_model.\")\n",
    "\n",
    "    if cfg.train.fine_tuning is True:\n",
    "        dnn_ckp_path = ''  # Fill in the checkpoint of host DNN.\n",
    "        dnn_ckp = torch.load(dnn_ckp_path)\n",
    "        Dnnet.load_state_dict(dnn_ckp[''])  # model weights\n",
    "        optimizerN.load_state_dict(dnn_ckp[''])  # parameters of the optimizer\n",
    "\n",
    "        logging.info(\"host DNN at checkpoint\" + '=' * 60)\n",
    "\n",
    "        pg_dict = dnn_ckp['optimizer_state_dict']['param_groups'][0]\n",
    "        logging.info(\"dnn_ckp: lr:{} momentum: {} weight_decay:{}\".format(pg_dict['lr'], pg_dict['momentum'], pg_dict['weight_decay']))\n",
    "        pg = optimizerN.param_groups[0]\n",
    "        logging.info(\"optimizerN: lr:{} momentum:{} weight_decay:{}\".format(pg['lr'], pg['momentum'], pg['weight_decay']))\n",
    "\n",
    "        logging.info(\"Epoch：{} val_loss：{:.4f} acc：{:.4%} prec：{:.4%} recall：{:.4%} f1：{:.4%}\".format(\n",
    "            dnn_ckp['epoch'], dnn_ckp['val_loss'], dnn_ckp['acc'], dnn_ckp['prec'], dnn_ckp['recall'], dnn_ckp['f1']))\n",
    "        \n",
    "        logging.info(\"host DNN at checkpoint\" + '=' * 60)\n",
    "        logging.info(\"Have loaded pretrained ResNet.\\n\")\n",
    "\n",
    "\n",
    "# model\n",
    "ste_model = End_to_end_Ste()\n",
    "# ste_model = GoogleNet()\n",
    "Dnnet = resnet18()\n",
    "ste_model.to(device)\n",
    "ste_model = nn.DataParallel(ste_model)\n",
    "Dnnet = nn.DataParallel(Dnnet.to(device))\n",
    "# optimization\n",
    "criterion_mse = nn.MSELoss()\n",
    "# Select appropriate hyper-parameters for specific steganography algorithm, especially the learning rate,\n",
    "# which will dramatically affect the performance of image hiding and extracting.\n",
    "optimizerH = torch.optim.Adam(ste_model.parameters(), lr=1e-3)\n",
    "schedulerH = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizerH, mode='min', factor=0.5)\n",
    "criterionN = nn.CrossEntropyLoss()\n",
    "optimizerN = optim.SGD(Dnnet.parameters(), lr=cfg.train.lr, momentum=cfg.train.momentum, weight_decay=cfg.train.weight_decay)\n",
    "schedulerN = torch.optim.lr_scheduler.StepLR(optimizerN, step_size=20, gamma=0.1)\n",
    "\n",
    "\n",
    "def train(epoch, train_cover_imgs, train_cover_img_labels, train_wms, train_wm_labels):\n",
    "\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    logging.info('\\nTraining epoch: %d' % epoch)\n",
    "\n",
    "    ste_model.train()\n",
    "    Dnnet.train()\n",
    "\n",
    "    step = 1\n",
    "    total_duration = 0\n",
    "    real_preds, real_trues, trigger_preds, trigger_trues, cover_preds, cover_trues = [], [], [], [], [], []\n",
    "    triggers = torch.Tensor()\n",
    "    # Save relevant metrics in dictionaries.\n",
    "    losses_dict = defaultdict(AverageMeter)\n",
    "    metrics_dict = defaultdict()\n",
    "\n",
    "    real_acc = Accuracy(num_classes=cfg.dataset.num_classes, average='weighted')\n",
    "    precision = Precision(num_classes=cfg.dataset.num_classes, average='weighted')\n",
    "    recall = Recall(num_classes=cfg.dataset.num_classes, average='weighted')\n",
    "    f1 = F1(num_classes=cfg.dataset.num_classes, average='weighted')\n",
    "    trigger_acc = Accuracy(num_classes=cfg.dataset.num_classes, average='weighted')\n",
    "    cover_acc = Accuracy(num_classes=cfg.dataset.num_classes, average='weighted')\n",
    "\n",
    "    for batch_idx, (input, label) in enumerate(train_loader):\n",
    "\n",
    "        input, label = input.to(device), label.to(device)\n",
    "        cover_img = train_cover_imgs[batch_idx % len(train_cover_imgs)]\n",
    "        cover_img_label = train_cover_img_labels[batch_idx % len(train_cover_img_labels)]\n",
    "        secret_img = train_wms[batch_idx % len(train_wms)]\n",
    "        trigger_label = train_wm_labels[batch_idx % len(train_wm_labels)]\n",
    "        \"\"\"############################### ste_model ###############################\"\"\"\n",
    "        trigger, trigger_ext_output = ste_model(cover_img, secret_img)\n",
    "        # Save all the triggers the last time they are traversed.\n",
    "        if batch_idx >= len(train_loader.dataset) / cfg.train.batchsize - len(train_wm_labels):\n",
    "            triggers = torch.cat([triggers, trigger.detach().cpu()], dim=0)\n",
    "        trigger_dnn_output = Dnnet(trigger.detach())\n",
    "        # loss for steganography\n",
    "        loss_hid = criterion_mse(cover_img, trigger)\n",
    "        loss_rev = criterion_mse(secret_img, trigger_ext_output)\n",
    "        # loss for host DNN and watermarking\n",
    "        loss_dnn = criterionN(trigger_dnn_output, trigger_label)\n",
    "        loss_H = cfg.train.loss_hyper_param[0] * loss_hid + cfg.train.loss_hyper_param[1] * loss_rev + cfg.train.loss_hyper_param[2] * loss_dnn\n",
    "\n",
    "        optimizerH.zero_grad()\n",
    "        loss_H.backward()\n",
    "        optimizerH.step()\n",
    "        \"\"\"############################### Dnnet ###############################\"\"\"\n",
    "        # It's extremely crucial to detach gradients here in case triggers are erroneously updated by back-propagation of host DNN.\n",
    "        inputs = torch.cat([input, trigger.detach()], dim=0)\n",
    "        labels = torch.cat([label, trigger_label], dim=0)\n",
    "\n",
    "        dnn_cat_output = Dnnet(inputs)\n",
    "        real_output = dnn_cat_output[0:cfg.train.batchsize]\n",
    "        trigger_output = dnn_cat_output[cfg.train.batchsize:]\n",
    "        cover_output = Dnnet(cover_img)\n",
    "\n",
    "        loss_cat_Dnn = criterionN(dnn_cat_output, labels)\n",
    "        loss_real = criterionN(real_output, label)\n",
    "        loss_trigger = criterionN(trigger_output, trigger_label)\n",
    "\n",
    "        optimizerN.zero_grad()\n",
    "        loss_cat_Dnn.backward()\n",
    "        optimizerN.step()\n",
    "        \"\"\"############################### metrics ###############################\"\"\"\n",
    "        real_pred = dnn_cat_output[0:cfg.train.batchsize].argmax(dim=1)\n",
    "        trigger_pred = dnn_cat_output[cfg.train.batchsize:].argmax(dim=1)\n",
    "        cover_pred = cover_output.argmax(dim=1)\n",
    "\n",
    "        real_preds.extend(real_pred.cpu().numpy())\n",
    "        real_trues.extend(label.cpu().numpy())\n",
    "        trigger_preds.extend(trigger_pred.cpu().numpy())\n",
    "        trigger_trues.extend(trigger_label.cpu().numpy())\n",
    "        cover_preds.extend(cover_pred.cpu().numpy())\n",
    "        cover_trues.extend(cover_img_label.cpu().numpy())\n",
    "\n",
    "        real_acc.update(real_pred.cpu(), label.cpu())\n",
    "        precision.update(real_pred.cpu(), label.cpu())\n",
    "        recall.update(real_pred.cpu(), label.cpu())\n",
    "        f1.update(real_pred.cpu(), label.cpu())\n",
    "        trigger_acc.update(trigger_pred.cpu(), trigger_label.cpu())\n",
    "        cover_acc.update(cover_pred.cpu(), cover_img_label.cpu())\n",
    "\n",
    "        temp_losses_dict = {\n",
    "            'loss_hid': loss_hid.item(),\n",
    "            'loss_rev': loss_rev.item(),\n",
    "            'loss_dnn': loss_dnn.item(),\n",
    "            'loss_H': loss_H.item(),\n",
    "            'loss_cat_Dnn': loss_cat_Dnn.item(),\n",
    "            'loss_real': loss_real.item(),\n",
    "            'loss_trigger': loss_trigger.item()\n",
    "        }\n",
    "        for tag, metric in temp_losses_dict.items():\n",
    "            if tag == 'loss_cat_Dnn':\n",
    "                losses_dict[tag].update(metric, inputs.size(0))\n",
    "            elif tag == 'loss_real':\n",
    "                losses_dict[tag].update(metric, input.size(0))\n",
    "            else:\n",
    "                losses_dict[tag].update(metric, trigger.size(0))\n",
    "\n",
    "        if step % cfg.train.print_freq == 0 or step == (len(train_loader)):\n",
    "            logging.info('[{}/{}][{}/{}] '\n",
    "                'Loss_H: {:.4f} (loss_hid: {:.4f} loss_rev: {:.4f} loss_dnn: {:.4f}) '\n",
    "                'Loss_cat_Dnn: {:.4f} Loss_real：{:.4f} Loss_trigger：{:.4f}'.format(epoch, cfg.train.num_epochs, step, len(train_loader),\n",
    "                    losses_dict['loss_H'].avg, losses_dict['loss_hid'].avg, losses_dict['loss_rev'].avg, \n",
    "                    losses_dict['loss_dnn'].avg, losses_dict['loss_cat_Dnn'].avg, losses_dict['loss_real'].avg, losses_dict['loss_trigger'].avg))\n",
    "\n",
    "            logging.info(\"Real acc: {:.4%} Trigger acc: {:.4%} Cover acc: {:.4%} \"\n",
    "                         \"Prec: {:.4%} Recall: {:.4%} F1: {:.4%}\".format(\n",
    "                             real_acc.compute(), trigger_acc.compute(), cover_acc.compute(), precision.compute(), recall.compute(), f1.compute()))\n",
    "            logging.info('-' * 130)\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    total_duration = time.time() - epoch_start_time\n",
    "    logging.info('Epoch {} total duration: {:.2f} sec'.format(epoch, total_duration))\n",
    "    logging.info('-' * 130)\n",
    "\n",
    "    metrics_dict['real_acc'] = real_acc.compute()\n",
    "    metrics_dict['precision'] = precision.compute()\n",
    "    metrics_dict['recall'] = recall.compute()\n",
    "    metrics_dict['f1'] = f1.compute()\n",
    "    metrics_dict['trigger_acc'] = trigger_acc.compute()\n",
    "    metrics_dict['cover_acc'] = cover_acc.compute()\n",
    "\n",
    "    write_scalars(epoch, os.path.join(run_folder, 'train.csv'), losses_dict, metrics_dict, None, total_duration)\n",
    "\n",
    "    save_cat_image(cfg, epoch, run_folder, cover_img, trigger, secret_img, trigger_ext_output, 'train')\n",
    "\n",
    "    return losses_dict, metrics_dict, triggers, train_wm_labels, trigger_ext_output\n",
    "\n",
    "\n",
    "def validation(epoch, val_cover_imgs, val_cover_img_labels, val_wms, val_wm_labels):\n",
    "    epoch_start_time = time.time()\n",
    "    logging.info('#' * 130)\n",
    "    logging.info('Running validation for epoch {}/{}'.format(epoch, cfg.train.num_epochs))\n",
    "\n",
    "    Dnnet.eval()\n",
    "    ste_model.eval()\n",
    "\n",
    "    total_duration = 0\n",
    "    real_preds, real_trues, trigger_preds, trigger_trues, cover_preds, cover_trues = [], [], [], [], [], []\n",
    "    triggers = torch.Tensor()\n",
    "    # Save relevant metrics in dictionaries.\n",
    "    losses_dict = defaultdict(AverageMeter)\n",
    "    metrics_dict = defaultdict()\n",
    "    img_quality_dict = defaultdict(AverageMeter)\n",
    "\n",
    "    real_acc = Accuracy(num_classes=cfg.dataset.num_classes, average='weighted')\n",
    "    precision = Precision(num_classes=cfg.dataset.num_classes, average='weighted')\n",
    "    recall = Recall(num_classes=cfg.dataset.num_classes, average='weighted')\n",
    "    f1 = F1(num_classes=cfg.dataset.num_classes, average='weighted')\n",
    "    trigger_acc = Accuracy(num_classes=cfg.dataset.num_classes, average='weighted')\n",
    "    cover_acc = Accuracy(num_classes=cfg.dataset.num_classes, average='weighted')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (input, label) in enumerate(val_loader):\n",
    "\n",
    "            input, label = input.to(device), label.to(device)\n",
    "            cover_img = val_cover_imgs[batch_idx % len(val_cover_imgs)]\n",
    "            cover_img_label = val_cover_img_labels[batch_idx % len(val_cover_img_labels)]\n",
    "            secret_img = val_wms[batch_idx % len(val_wms)]\n",
    "            trigger_label = val_wm_labels[batch_idx % len(val_wm_labels)]\n",
    "            \"\"\"############################### ste_model ###############################\"\"\"\n",
    "            trigger, trigger_ext_output = ste_model(cover_img, secret_img)\n",
    "            if batch_idx >= len(val_loader.dataset) / cfg.train.batchsize - len(val_wm_labels):\n",
    "                triggers = torch.cat([triggers, trigger.detach().cpu()], dim=0)\n",
    "            trigger_dnn_output = Dnnet(trigger.detach())\n",
    "            # loss for steganography\n",
    "            loss_hid = criterion_mse(cover_img, trigger)\n",
    "            loss_rev = criterion_mse(secret_img, trigger_ext_output)\n",
    "            # loss for host DNN and watermarking\n",
    "            loss_dnn = criterionN(trigger_dnn_output, trigger_label)\n",
    "            loss_H = cfg.train.loss_hyper_param[0] * loss_hid + cfg.train.loss_hyper_param[1] * loss_rev + cfg.train.loss_hyper_param[2] * loss_dnn\n",
    "            \"\"\"############################### Dnnet ###############################\"\"\"\n",
    "            inputs = torch.cat([input, trigger.detach()], dim=0)\n",
    "            labels = torch.cat([label, trigger_label], dim=0)\n",
    "\n",
    "            dnn_cat_output = Dnnet(inputs)\n",
    "            real_output = dnn_cat_output[0:cfg.train.batchsize]\n",
    "            trigger_output = dnn_cat_output[cfg.train.batchsize:]\n",
    "            cover_output = Dnnet(cover_img)\n",
    "\n",
    "            loss_cat_Dnn = criterionN(dnn_cat_output, labels)\n",
    "            loss_real = criterionN(real_output, label)\n",
    "            loss_trigger = criterionN(trigger_output, trigger_label)\n",
    "            \"\"\"############################### metrics ###############################\"\"\"\n",
    "            real_pred = dnn_cat_output[0:cfg.train.batchsize].argmax(dim=1)\n",
    "            trigger_pred = dnn_cat_output[cfg.train.batchsize:].argmax(dim=1)\n",
    "            cover_pred = cover_output.argmax(dim=1)\n",
    "\n",
    "            real_preds.extend(real_pred.cpu().numpy())\n",
    "            real_trues.extend(label.cpu().numpy())\n",
    "            trigger_preds.extend(trigger_pred.cpu().numpy())\n",
    "            trigger_trues.extend(trigger_label.cpu().numpy())\n",
    "            cover_preds.extend(cover_pred.cpu().numpy())\n",
    "            cover_trues.extend(cover_img_label.cpu().numpy())\n",
    "\n",
    "            real_acc.update(real_pred.cpu(), label.cpu())\n",
    "            precision.update(real_pred.cpu(), label.cpu())\n",
    "            recall.update(real_pred.cpu(), label.cpu())\n",
    "            f1.update(real_pred.cpu(), label.cpu())\n",
    "            trigger_acc.update(trigger_pred.cpu(), trigger_label.cpu())\n",
    "            cover_acc.update(cover_pred.cpu(), cover_img_label.cpu())\n",
    "\n",
    "            temp_losses_dict = {\n",
    "                'loss_hid': loss_hid.item(),\n",
    "                'loss_rev': loss_rev.item(),\n",
    "                'loss_dnn': loss_dnn.item(),\n",
    "                'loss_H': loss_H.item(),\n",
    "                'loss_cat_Dnn': loss_cat_Dnn.item(),\n",
    "                'loss_real': loss_real.item(),\n",
    "                'loss_trigger': loss_trigger.item()\n",
    "            }\n",
    "            for tag, metric in temp_losses_dict.items():\n",
    "                if tag == 'loss_cat_Dnn':\n",
    "                    losses_dict[tag].update(metric, inputs.size(0))\n",
    "                elif tag == 'loss_real':\n",
    "                    losses_dict[tag].update(metric, input.size(0))\n",
    "                else:\n",
    "                    losses_dict[tag].update(metric, trigger.size(0))\n",
    "\n",
    "            ste_psnr = cal_psnr(cover_img, trigger.detach())\n",
    "            rev_psnr = cal_psnr(secret_img, trigger_ext_output.detach())\n",
    "            img_quality_dict['ste_psnr'].update(ste_psnr, trigger.size(0))\n",
    "            img_quality_dict['rev_psnr'].update(rev_psnr, trigger_ext_output.size(0))\n",
    "\n",
    "    logging.info(\n",
    "        '[{}/{}] Loss_H: {:.4f} (loss_hid: {:.4f} loss_rev: {:.4f} loss_dnn: {:.4f}) Loss_cat_Dnn: {:.4f}  Loss_real：{:.4f} Loss_trigger：{:.4f}'.format(\n",
    "            epoch, cfg.train.num_epochs,\n",
    "            losses_dict['loss_H'].avg, losses_dict['loss_hid'].avg, losses_dict['loss_rev'].avg, losses_dict['loss_dnn'].avg,  losses_dict['loss_cat_Dnn'].avg,\n",
    "            losses_dict['loss_real'].avg, losses_dict['loss_trigger'].avg))\n",
    "    logging.info(\"Real acc: {:.4%} Trigger acc: {:.4%} Cover acc: {:.4%} \"\n",
    "                 \"Precision: {:.4%} Recall: {:.4%} F1: {:.4%} ste_psnr: {:.2f} rev_psnr: {:.2f}\".format(real_acc.compute(), trigger_acc.compute(), cover_acc.compute(),\n",
    "                                                                                                        precision.compute(), recall.compute(), f1.compute(), ste_psnr, rev_psnr))\n",
    "\n",
    "    total_duration = time.time() - epoch_start_time\n",
    "    logging.info('Epoch {} total duration: {:.2f} sec'.format(epoch, total_duration))\n",
    "    logging.info('#' * 130)\n",
    "\n",
    "    metrics_dict['real_acc'] = real_acc.compute()\n",
    "    metrics_dict['precision'] = precision.compute()\n",
    "    metrics_dict['recall'] = recall.compute()\n",
    "    metrics_dict['f1'] = f1.compute()\n",
    "    metrics_dict['trigger_acc'] = trigger_acc.compute()\n",
    "    metrics_dict['cover_acc'] = cover_acc.compute()\n",
    "\n",
    "    write_scalars(epoch, os.path.join(run_folder, 'val.csv'), losses_dict, metrics_dict, img_quality_dict, total_duration)\n",
    "    save_cat_image(cfg, epoch, run_folder, cover_img, trigger, secret_img, trigger_ext_output, 'val')\n",
    "\n",
    "    return losses_dict, metrics_dict, img_quality_dict, triggers, val_wm_labels, trigger_ext_output\n",
    "\n",
    "\n",
    "def test(test_cover_imgs, test_cover_img_labels, test_wms, test_wm_labels):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    Dnnet.eval()\n",
    "    ste_model.eval()\n",
    "\n",
    "    test_duration = 0\n",
    "    real_preds, real_trues, trigger_preds, trigger_trues, cover_preds, cover_trues = [], [], [], [], [], []\n",
    "    triggers = torch.Tensor()\n",
    "    # Save relevant metrics in dictionaries.\n",
    "    losses_dict = defaultdict(AverageMeter)\n",
    "    metrics_dict = defaultdict()\n",
    "    img_quality_dict = defaultdict(AverageMeter)\n",
    "\n",
    "    real_acc = Accuracy(num_classes=cfg.dataset.num_classes, average='weighted')\n",
    "    precision = Precision(num_classes=cfg.dataset.num_classes, average='weighted')\n",
    "    recall = Recall(num_classes=cfg.dataset.num_classes, average='weighted')\n",
    "    f1 = F1(num_classes=cfg.dataset.num_classes, average='weighted')\n",
    "    trigger_acc = Accuracy(num_classes=cfg.dataset.num_classes, average='weighted')\n",
    "    cover_acc = Accuracy(num_classes=cfg.dataset.num_classes, average='weighted')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (input, label) in enumerate(test_loader):\n",
    "            input, label = input.to(device), label.to(device)\n",
    "            cover_img = test_cover_imgs[batch_idx % len(test_cover_imgs)]\n",
    "            cover_img_label = test_cover_img_labels[batch_idx % len(test_cover_img_labels)]\n",
    "            secret_img = test_wms[batch_idx % len(test_wms)]\n",
    "            trigger_label = test_wm_labels[batch_idx % len(test_wm_labels)]\n",
    "            \"\"\"############################### ste_model ###############################\"\"\"\n",
    "            trigger, trigger_ext_output = ste_model(cover_img, secret_img)\n",
    "            if batch_idx >= len(val_loader.dataset) / cfg.train.batchsize - len(val_wm_labels):\n",
    "                triggers = torch.cat([triggers, trigger.detach().cpu()], dim=0)\n",
    "            trigger_dnn_output = Dnnet(trigger.detach())\n",
    "            # loss for steganography\n",
    "            loss_hid = criterion_mse(cover_img, trigger.detach())\n",
    "            loss_rev = criterion_mse(secret_img, trigger_ext_output.detach())\n",
    "            # loss for host DNN and watermarking\n",
    "            loss_dnn = criterionN(trigger_dnn_output, trigger_label)\n",
    "            loss_H = cfg.train.loss_hyper_param[0] * loss_hid + cfg.train.loss_hyper_param[1] * loss_rev + cfg.train.loss_hyper_param[2] * loss_dnn\n",
    "            \"\"\"############################### Dnnet ###############################\"\"\"\n",
    "            inputs = torch.cat([input, trigger.detach()], dim=0)\n",
    "            labels = torch.cat([label, trigger_label], dim=0)\n",
    "\n",
    "            dnn_cat_output = Dnnet(inputs)\n",
    "            real_output = dnn_cat_output[0:cfg.train.batchsize]\n",
    "            trigger_output = dnn_cat_output[cfg.train.batchsize:]\n",
    "            cover_output = Dnnet(cover_img)\n",
    "\n",
    "            loss_cat_Dnn = criterionN(dnn_cat_output, labels)\n",
    "            loss_real = criterionN(real_output, label)\n",
    "            loss_trigger = criterionN(trigger_output, trigger_label)\n",
    "            \"\"\"############################### metrics ###############################\"\"\"\n",
    "            real_pred = dnn_cat_output[0:cfg.train.batchsize].argmax(dim=1)\n",
    "            trigger_pred = dnn_cat_output[cfg.train.batchsize:].argmax(dim=1)\n",
    "            cover_pred = cover_output.argmax(dim=1)\n",
    "\n",
    "            real_preds.extend(real_pred.cpu().numpy())\n",
    "            real_trues.extend(label.cpu().numpy())\n",
    "            trigger_preds.extend(trigger_pred.cpu().numpy())\n",
    "            trigger_trues.extend(trigger_label.cpu().numpy())\n",
    "            cover_preds.extend(cover_pred.cpu().numpy())\n",
    "            cover_trues.extend(cover_img_label.cpu().numpy())\n",
    "\n",
    "            real_acc.update(real_pred.cpu(), label.cpu())\n",
    "            precision.update(real_pred.cpu(), label.cpu())\n",
    "            recall.update(real_pred.cpu(), label.cpu())\n",
    "            f1.update(real_pred.cpu(), label.cpu())\n",
    "            trigger_acc.update(trigger_pred.cpu(), trigger_label.cpu())\n",
    "            cover_acc.update(cover_pred.cpu(), cover_img_label.cpu())\n",
    "\n",
    "            temp_losses_dict = {\n",
    "                'loss_hid': loss_hid.item(),\n",
    "                'loss_rev': loss_rev.item(),\n",
    "                'loss_dnn': loss_dnn.item(),\n",
    "                'loss_H': loss_H.item(),\n",
    "                'loss_cat_Dnn': loss_cat_Dnn.item(),\n",
    "                'loss_real': loss_real.item(),\n",
    "                'loss_trigger': loss_trigger.item()\n",
    "            }\n",
    "            for tag, metric in temp_losses_dict.items():\n",
    "                if tag == 'loss_cat_Dnn':\n",
    "                    losses_dict[tag].update(metric, inputs.size(0))\n",
    "                elif tag == 'loss_real':\n",
    "                    losses_dict[tag].update(metric, input.size(0))\n",
    "                else:\n",
    "                    losses_dict[tag].update(metric, trigger.size(0))\n",
    "\n",
    "            ste_psnr = cal_psnr(cover_img, trigger.detach())\n",
    "            rev_psnr = cal_psnr(secret_img, trigger_dnn_output.detach())\n",
    "            img_quality_dict['ste_psnr'].update(ste_psnr, trigger.size(0))\n",
    "            img_quality_dict['rev_psnr'].update(rev_psnr, trigger_dnn_output.size(0))\n",
    "\n",
    "    logging.info(\n",
    "        'Loss_H: {:.4f} (loss_hid: {:.4f} loss_rev: {:.4f} dnn: {:.4f}) '\n",
    "        'Loss_cat_Dnn: {:.4f} Loss_real：{:.4f} Loss_trigger：{:.4f}'.format(\n",
    "            losses_dict['loss_H'].avg, losses_dict['loss_hid'].avg, losses_dict['loss_rev'].avg, losses_dict['loss_dnn'].avg,\n",
    "            losses_dict['loss_cat_Dnn'].avg, losses_dict['loss_real'].avg, losses_dict['loss_trigger'].avg))\n",
    "    logging.info(\"Real acc: {:.4%} Trigger acc: {:.4%} Cover acc: {:.4%} \"\n",
    "                 \"Precision: {:.4%} Recall: {:.4%} F1: {:.4%} ste_psnr: {:.2f} rev_psnr: {:.2f}\".format(real_acc.compute(), trigger_acc.compute(), cover_acc.compute(), \n",
    "                                                                                   precision.compute(), precision.compute(), recall.compute(), f1.compute(), ste_psnr, rev_psnr))\n",
    "\n",
    "    test_duration = time.time() - epoch_start_time\n",
    "    logging.info('test duration {:.2f} sec'.format(test_duration))\n",
    "\n",
    "    metrics_dict['real_acc'] = real_acc.compute()\n",
    "    metrics_dict['precision'] = precision.compute()\n",
    "    metrics_dict['recall'] = recall.compute()\n",
    "    metrics_dict['f1'] = f1.compute()\n",
    "    metrics_dict['trigger_acc'] = trigger_acc.compute()\n",
    "    metrics_dict['cover_acc'] = cover_acc.compute()\n",
    "\n",
    "    write_scalars(1, os.path.join(run_folder, 'test.csv'), losses_dict, metrics_dict, img_quality_dict, test_duration)\n",
    "    save_cat_image(cfg, 1, run_folder, cover_img, trigger, secret_img, trigger_ext_output, 'test')\n",
    "    save_separate_image(1, run_folder, triggers, test_wm_labels, trigger_ext_output, 'test')\n",
    "\n",
    "    plot_confusion_matrix(1, run_folder, 'test_real', real_preds, real_trues, test_loader)\n",
    "    plot_confusion_matrix(1, run_folder, 'test_trigger', trigger_preds, trigger_trues, None)\n",
    "\n",
    "\n",
    "def main():\n",
    "    min_cat_loss = np.Inf\n",
    "    min_cat_loss_epoch = 0\n",
    "\n",
    "    # Early stop the training according to real_loss, trigger_loss and cat_loss respectively.\n",
    "    early_real_loss = E2E_EarlyStopping(patience=cfg.train.es_patience, verbose=True, trace_func=logging.info)\n",
    "    early_trigger_loss = E2E_EarlyStopping(patience=cfg.train.es_patience, verbose=True, trace_func=logging.info)\n",
    "    early_cat_loss = E2E_EarlyStopping(patience=cfg.train.es_patience, verbose=True, trace_func=logging.info)\n",
    "    # whether follow the early stopping mechanism\n",
    "    real_go_on, trigger_go_on, cat_go_on = True, True, True\n",
    "    # DO NOT load checkpoints, since E2E-Extraction requires to train from scratch.\n",
    "    # load_pretrained()\n",
    "\n",
    "    for epoch in range(cfg.train.start_epoch, cfg.train.num_epochs + 1):\n",
    "        # To shuffle the datasets, cover images and secret images should be reloaded every epoch.\n",
    "        train_cover_imgs, train_cover_img_labels, train_wms, train_wm_labels = load_cover_and_wm(cfg, 'cover_train')\n",
    "        val_cover_imgs, val_cover_img_labels, val_wms, val_wm_labels = load_cover_and_wm(cfg, 'cover_val')\n",
    "\n",
    "        logging.info(\"\\ntrain_cover_img_labels:{} \\ntrain_cover_img_labels[0]:{} \\ntrain_wm_labels:{} \\ntrain_wm_labels[0]:{}\".format(len(train_cover_img_labels), train_cover_img_labels[0], len(train_wm_labels), train_wm_labels[0]))\n",
    "        logging.info(\"val_cover_img_labels:{} \\nval_cover_img_labels[0]:{} \\nval_wm_labels:{} \\nval_wm_labels[0]:{}\".format(len(val_cover_img_labels), val_cover_img_labels[0], len(val_wm_labels), val_wm_labels[0]))\n",
    "\n",
    "        train_losses_dict, train_metrics_dict, train_triggers, train_trigger_labels, train_ext = train(epoch, train_cover_imgs, train_cover_img_labels, train_wms, train_wm_labels)\n",
    "\n",
    "        val_losses_dict, val_metrics_dict, img_quality_dict, val_triggers, val_trigger_labels, val_ext = validation(epoch, val_cover_imgs, val_cover_img_labels, val_wms, val_wm_labels)\n",
    "        logging.info(\"Learning rate: resnet:{} stegan_model:{}\".format(optimizerN.param_groups[0]['lr'], optimizerH.param_groups[0]['lr']))\n",
    "        plot_scalars(epoch, run_folder, train_losses_dict, train_metrics_dict, val_losses_dict, val_metrics_dict, img_quality_dict)\n",
    "\n",
    "        logging.info(\"train_triggers:{} val_triggers:{}\".format(len(train_triggers), len(val_triggers)))\n",
    "\n",
    "        schedulerH.step(val_losses_dict['loss_H'].avg)\n",
    "        schedulerN.step()\n",
    "\n",
    "        # Save training sets and validation sets of triggers according to loss_cat_Dnn.\n",
    "        if val_losses_dict['loss_cat_Dnn'].avg < min_cat_loss:\n",
    "            min_cat_loss = val_losses_dict['loss_cat_Dnn'].avg\n",
    "            min_cat_loss_epoch = epoch\n",
    "\n",
    "            save_separate_image(epoch, run_folder, train_triggers, train_trigger_labels, train_ext, 'train')\n",
    "            save_separate_image(epoch, run_folder, val_triggers, val_trigger_labels, val_ext, 'val')\n",
    "\n",
    "        if real_go_on is True:\n",
    "            early_real_loss(val_losses_dict['loss_real'].avg, epoch, run_folder,\n",
    "                            ste_model, Dnnet, optimizerH, optimizerN,\n",
    "                            val_losses_dict, val_metrics_dict, img_quality_dict, 'loss_real')\n",
    "            if early_real_loss.early_stop:\n",
    "                logging.info(\"The training has gained minimum real loss at {}th epoch\".format(epoch))\n",
    "                real_go_on = False\n",
    "\n",
    "        if trigger_go_on is True:\n",
    "            early_trigger_loss(val_losses_dict['loss_trigger'].avg, epoch, run_folder,\n",
    "                            ste_model, Dnnet, optimizerH, optimizerN,\n",
    "                            val_losses_dict, val_metrics_dict, img_quality_dict, 'loss_trigger')\n",
    "            if early_trigger_loss.early_stop:\n",
    "                logging.info(\"The training has gained minimum trigger loss at {}th epoch\".format(epoch))\n",
    "                trigger_go_on = False\n",
    "\n",
    "        if cat_go_on is True:\n",
    "            early_cat_loss(val_losses_dict['loss_cat_Dnn'].avg, epoch, run_folder,\n",
    "                            ste_model, Dnnet, optimizerH, optimizerN,\n",
    "                            val_losses_dict, val_metrics_dict, img_quality_dict, 'loss_cat_Dnn')\n",
    "            if early_cat_loss.early_stop:\n",
    "                logging.info(\"The training has gained minimum cat loss at {}th epoch\".format(epoch))\n",
    "                cat_go_on = False\n",
    "        # Due to unexpected fluctuations during training, every checkpoint is expected to be saved despite the early stopping mechanism.\n",
    "        save_all_models(epoch, run_folder, ste_model, Dnnet, optimizerH, optimizerN,\n",
    "                        val_losses_dict, val_metrics_dict, img_quality_dict, None, None)\n",
    "\n",
    "\n",
    "    logging.info(\"################## Finished ##################\")\n",
    "\n",
    "    if cfg.test is True:\n",
    "        logging.info(\"################## Testing... ##################\")\n",
    "\n",
    "        E2E_pt_root = ''  # where you store the checkpoints\n",
    "        E2E_ste_path, E2E_dnn_path = os.path.join(E2E_pt_root, ''), os.path.join(E2E_pt_root, '')  # Fill in paths of pretrained steganography and host model respectively.\n",
    "        E2E_ste, E2E_dnn = torch.load(E2E_ste_path), torch.load(E2E_dnn_path)\n",
    "        # model weights\n",
    "        ste_model.load_state_dict(E2E_ste[''])\n",
    "        Dnnet.load_state_dict(E2E_dnn[''])\n",
    "        logging.info(\"Have loaded ste_model checkpoint from '{}'\".format(E2E_pt_root))\n",
    "\n",
    "        logging.info(\"E2E-Extraction models at checkpoint\" + '=' * 60)\n",
    "        logging.info(\"Loss_H: {:.4f} (loss_hid: {:.4f} loss_rev: {:.4f} loss_dnn: {:.4f}) Loss_cat_Dnn: {:.4f} Loss_real：{:.4f} Loss_trigger：{:.4f}\".format(\n",
    "            E2E_ste['loss_H'], E2E_ste['loss_hid'], E2E_ste['loss_rev'], E2E_ste['loss_dnn'], E2E_dnn['loss_cat_Dnn'], E2E_dnn['loss_real'], E2E_dnn['loss_trigger']))\n",
    "        logging.info(\"Real acc: {:.4%} Trigger acc: {:.4%} Cover acc: {:.4%} Precision: {:.4%} Recall: {:.4%} F1: {:.4%} ste_psnr: {:.2f} rev_psnr: {:.2f}\".format(\n",
    "            E2E_dnn['real_acc'], E2E_dnn['trigger_acc'], E2E_dnn['cover_acc'], E2E_dnn['precision'], E2E_dnn['recall'], E2E_dnn['f1'], E2E_ste['ste_psnr'], E2E_ste['rev_psnr']))\n",
    "        logging.info(\"E2E-Extraction models at checkpoint\" + '=' * 60)\n",
    "\n",
    "        test_cover_imgs, test_cover_img_labels, test_wms, test_wm_labels = load_cover_and_wm(cfg, 'cover_test')\n",
    "        logging.info(\"test_cover_img_labels:{} test_cover_img_labels[0]:{} test_wm_labels:{} test_wm_labels[0]:{}\".format(len(test_cover_img_labels), test_cover_img_labels[0], len(test_wm_labels), test_wm_labels[0]))\n",
    "        test(test_cover_imgs, test_cover_img_labels, test_wms, test_wm_labels)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
